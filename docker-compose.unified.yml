services:
  redis:
    image: redis:7-alpine
    # Redis streams are used for event distribution from firehose reader to consumer worker
    # Using noeviction policy to prevent data loss
    command: redis-server --maxmemory 8gb --maxmemory-policy noeviction --appendonly yes --appendfsync everysec
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  db:
    image: postgres:14
    # Reduced max_connections since we only have 1 worker now (not 32)
    command: postgres -c max_connections=500 -c shared_buffers=20GB -c effective_cache_size=42GB -c work_mem=256MB -c maintenance_work_mem=8GB -c max_parallel_workers=32 -c max_parallel_workers_per_gather=8 -c max_wal_size=8GB
    environment:
      POSTGRES_DB: atproto
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data,Z
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d atproto"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    shm_size: 10gb

  # Python Firehose Reader - Connects to firehose and writes to Redis
  python-firehose:
    build: ./python-firehose
    environment:
      - RELAY_URL=${RELAY_URL:-wss://bsky.network}
      - REDIS_URL=redis://redis:6379
      - REDIS_STREAM_KEY=firehose:events
      - REDIS_CURSOR_KEY=firehose:python_cursor
      - REDIS_MAX_STREAM_LEN=500000
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import redis; r = redis.from_url('redis://redis:6379'); r.ping()\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  # PYTHON REDIS CONSUMER - Replaces 32 TypeScript workers!
  # Single Python process that:
  # - Consumes events from Redis streams (XREADGROUP)
  # - Processes events to PostgreSQL
  # - Runs 5 parallel consumer pipelines
  python-worker:
    build:
      context: ./python-firehose
      dockerfile: Dockerfile.worker
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - REDIS_STREAM_KEY=firehose:events
      - REDIS_CONSUMER_GROUP=firehose-processors
      - CONSUMER_ID=python-worker
      - DB_POOL_SIZE=20
      - BATCH_SIZE=10
      - PARALLEL_CONSUMERS=5
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      app:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import asyncio; import redis.asyncio as redis; asyncio.run(redis.from_url('redis://redis:6379').ping())\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    # Resource limits (much lower than 32 workers!)
    deploy:
      resources:
        limits:
          memory: 4G  # Single Python process uses way less than 32 Node.js workers
        reservations:
          memory: 1G

  # Python Unified Worker with Backfill Support
  # Connects directly to firehose and processes to PostgreSQL with optional historical backfill
  # Set BACKFILL_DAYS environment variable to enable: 0=disabled, -1=all history, >0=specific days
  python-backfill-worker:
    build:
      context: ./python-firehose
      dockerfile: Dockerfile.unified
    environment:
      - RELAY_URL=${RELAY_URL:-wss://bsky.network}
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - DB_POOL_SIZE=20
      - LOG_LEVEL=INFO
      # Backfill configuration - Set BACKFILL_DAYS to enable automatic backfill
      - BACKFILL_DAYS=${BACKFILL_DAYS:-0}
      - BACKFILL_BATCH_SIZE=${BACKFILL_BATCH_SIZE:-5}
      - BACKFILL_BATCH_DELAY_MS=${BACKFILL_BATCH_DELAY_MS:-2000}
      - BACKFILL_MAX_CONCURRENT=${BACKFILL_MAX_CONCURRENT:-2}
      - BACKFILL_MAX_MEMORY_MB=${BACKFILL_MAX_MEMORY_MB:-512}
      - BACKFILL_USE_IDLE=${BACKFILL_USE_IDLE:-true}
      - BACKFILL_DB_POOL_SIZE=${BACKFILL_DB_POOL_SIZE:-2}
      # Worker ID (backfill only runs on worker 0)
      - WORKER_ID=0
    depends_on:
      db:
        condition: service_healthy
      app:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import asyncpg; import asyncio; asyncio.run(asyncpg.connect('postgresql://postgres:password@db:5432/atproto', timeout=5).close())\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # Frontend/API server - Lightweight since it's not processing firehose
  app:
    volumes:
      - ./appview-signing-key.json:/app/appview-signing-key.json:ro,Z
      - ./appview-private.pem:/app/appview-private.pem:ro,Z
      - ./public/did.json:/app/public/did.json:ro,Z
      - ./oauth-keyset.json:/app/oauth-keyset.json:ro,Z
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - REDIS_URL=redis://redis:6379
      - SESSION_SECRET=${SESSION_SECRET:-change-this-to-a-random-secret-in-production}
      - APPVIEW_DID=${APPVIEW_DID:-did:web:appview.dollspace.gay}
      # TypeScript backfill is PERMANENTLY DISABLED - use Python backfill instead
      # See python-firehose/backfill_service.py
      - BACKFILL_DAYS=0  # Force disabled - TypeScript backfill removed
      - DATA_RETENTION_DAYS=${DATA_RETENTION_DAYS:-0}
      - DB_POOL_SIZE=50  # Reduced from 200 since no firehose processing
      - PORT=5000
      - NODE_ENV=production
      - OAUTH_KEYSET_PATH=/app/oauth-keyset.json
      - ADMIN_DIDS=did:plc:abc123xyz,admin.bsky.social,did:plc:def456uvw
      - FIREHOSE_ENABLED=false  # Disable TypeScript firehose - Python handles it
      - CONSTELLATION_ENABLED=true
      - CONSTELLATION_URL=https://constellation.microcosm.blue
      - CONSTELLATION_TIMEOUT=15000
      - CONSTELLATION_CACHE_TTL=${CONSTELLATION_CACHE_TTL:-60}
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:5000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\""]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    # Resource limits - much lower since it's just serving API/UI
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  # Constellation Bridge - Optional enhanced statistics
  constellation-bridge:
    build: ./microcosm-bridge/constellation-client
    environment:
      - CONSTELLATION_URL=${CONSTELLATION_URL:-https://constellation.microcosm.blue}
      - CONSTELLATION_TIMEOUT=5000
      - REDIS_URL=redis://redis:6379
      - CACHE_ENABLED=true
      - CACHE_TTL=${CONSTELLATION_CACHE_TTL:-60}
      - HEALTH_PORT=3003
      - MAX_REQUESTS_PER_SECOND=10
      - USER_AGENT=AppView-Constellation-Bridge/1.0 (@appview.bsky.social)
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3003/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\""]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped

volumes:
  postgres_data:
